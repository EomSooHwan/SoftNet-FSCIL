#################
# General Setting
#################
# Experiment name, if the experiment name has the word debug, it will enter debug mode
<<<<<<< HEAD
name: FSIL_CFRPModel_res18_CUB_baseline_100base_f2m_batch256_last8_time2_sgd_color_used_v2_c0.93_lr2e-3_clr2e-2
#name: FSIL_CFRPModel_res18_CUB_HardNet_100base_f2m_last8_time4_bound2_adam_color_v1_c0.93_lr5e-5_clr1e-3_mile40_120_100fc_220epoch
=======
# Hardnet-baseline
name: FSIL_CFRPModel_res18_CUB_baseline_100base_WSN0.90
#name: FSIL_CFRPModel_res18_CUB_baseline_100base_wm1e-2_WSN0.90_signet0.60_200epoch
>>>>>>> origin/gpu219
# The type of model used, usually the class name of the model defined in the `methods` directory
model_type: WSNBaseModel
# gpu
gpu: 0
# Random seed
manual_seed: 1997
Random: false

#################################
# The settings for the dataset and data loader
#################################
transformer_agu:
  - type: RandomResizedCrop
    size: 224
  - type: ColorJitter
    brightness: 0.4
    contrast: 0.4
    saturation: 0.4
  - type: RandomHorizontalFlip
  - type: ToTensor
  - type: Normalize
    mean: !!python/tuple [0.485, 0.456, 0.406]
    std: !!python/tuple [0.229, 0.224, 0.225]

transformer:
  - type: Resize
    size: 256
  - type: CenterCrop
    size: 224
  - type: ToTensor
  - type: Normalize
    mean: !!python/tuple [ 0.485, 0.456, 0.406 ]
    std: !!python/tuple [ 0.229, 0.224, 0.225 ]


datasets:
  # The important information of dataset
  train:
    name: cub
    type: NormalDataset
    total_classes: 200
    dataroot: DataSet/CUB_200_2011/train
    aug: true
    # Number of processes read by the data loader per GPU
    num_worker_per_gpu: 4
    # Batch size
    batch_size_base_classes: 256
    pin_memory: true
    pre_load: false

    sampler:
      type: TaskSampler
      num_samples: 8

  val:
    name: cub
    type: NormalDataset
    total_classes: 200
    dataroot: DataSet/CUB_200_2011/test
    aug: false
    # Number of processes read by the data loader per GPU
    num_worker_per_gpu: 4
    pin_memory: true
    pre_load: false
    sampler:
      type: ~

#####################
# The following is the setup of the network structure
#####################
network_g:
  type: SigResnet18_small
  num_classes: 100
  adopt_classifier: true
  flatten: true
<<<<<<< HEAD
  sparsity: 0.07
=======
  sparsity: 0.1
>>>>>>> origin/gpu219
  gamma: 1.0

######################################
# The following are the paths
######################################
path:
  # Pre-trained model path, need models ending in pth
<<<<<<< HEAD
  #pretrain_model_g: ./exp/cub_bases1000/subnet-fc_c0.001_bs512_Adam_lr0.001/checkpoints/last.pth
  #pretrain_model_g: ./exp/cub_bases1000/subnet-2gpus-fc_c0.001_bs512_Adam_lr0.001/checkpoints/last.pth
  pretrain_model_g: ./exp/cub_bases1000/subnet-2gpus-all-fc_c0.001_bs512_Adam_lr0.001_alpha1.0/checkpoints/last.pth
=======
  pretrain_model_g: ./exp_cub/
>>>>>>> origin/gpu219
  # Whether to load pretrained models strictly, that is the corresponding parameter names should be the same
  strict_load: false
  #
  resume_state: ~


#################
# The following is the training setup
#################
train:
  # Optimizer
<<<<<<< HEAD
  # F2M
  proto_loss:
    type: ProtoFixLoss
    shots: 8
    w: 1.0

  random_noise:
    random_times: 2
    type: suffix_only_conv_weight
    num_layers: 8
    low: 0.1
    high: 3.0
    reduction_factor: 4
    bound_value: 0.0002
    distribution:
      type: DiscreteBeta
=======
  optim_g:
    # The type of optimizer
    type: iSGD
    # learning rate
    lr: !!float 1e-3
    weight_decay: !!float 5e-4
    # momentum
    momentum: !!float 0.9
>>>>>>> origin/gpu219

  optim_g:
    # The type of optimizer
    type: SGD
    # learning rate
<<<<<<< HEAD
    lr: !!float 2e-2
    lr_cf: !!float 2e-2 
=======
    lr: !!float 1e-3
>>>>>>> origin/gpu219
    weight_decay: !!float 5e-4
    # momentum
    momentum: !!float 0.9

  # The setting of scheduler
  scheduler:
    # The type of scheduler
    type: MultiStepLR
    #### The following properties are flexible, depending on the learning rate Scheduler has different settings
    # milestones, b128
    #milestones: [!!python/object/apply:eval [ 600 * 3 * 2 ],
    #             !!python/object/apply:eval [ 900 * 3 * 2 ]]
    # milestones, b256
    milestones: [!!python/object/apply:eval [ 600 * 3 ],
                 !!python/object/apply:eval [ 900 * 3 ]]
    # gama
    gamma: 0.1

  # The number of epoch
<<<<<<< HEAD
  epoch: 1200
  s_epoch: 550 # start smoothing
=======
  epoch: 220
  s_epoch: 230 # start smoothing
>>>>>>> origin/gpu219
  # The number of warm up iteration, if -1, means no warm up
  warmup_iter: -1  # no warm up
  # number of base classes
  bases: 100
  # number of tasks for incremental few-shot learning
  tasks: 11

#######################
# The following are the settings for Validation
#######################
val:
  # The frequency of validation
  val_freq: 200

####################
# The following are the settings for Logging
####################
logger:
  # Frequency of loggers printed on the screen according to iteration
  print_freq: 10
  # 保存checkpoint的频率
  save_checkpoint_freq: 15000
  # Whether to use tensorboard logger

  use_tb_logger: true
  # Whether to use wandb logger
  wandb:
    # The default is None, i.e. wandb is not used.
    project: FSIL-Cub-100bases-noBuffer
    # If it is resume, you can enter the last wandb id, then the log can be connected
    resume_id: ~


